# Stage 2

## æ˜ç¡®è¦æ±‚ï¼š

* ä¸è¦ä¿®æ”¹æç¤ºè¯ã€ä¸åŠ å™ª
* æ”¹æºç ï¼Œè®©æ¨¡å‹èƒ½è®°ä½ç‹—å’Œé¸Ÿï¼Œä½†æ˜¯å¿˜è®°çŒ«

## Temp 1: åœ¨ä¸­é—´æˆªèƒ¡æ•°æ®æµ

### 1.è¯†åˆ«ç‰¹å¾å‘é‡

å…ˆæ‰¾åˆ°è¯†åˆ«çŒ«çš„ç‰¹å¾å‘é‡ å¹¶ç›´æ¥å‚¨å­˜ è¿™é‡Œç›¸å½“äºå‘Šè¯‰æˆ‘ä»¬åç»­çš„â€œæ‰‹æœ¯â€æˆ‘ä»¬åº”è¯¥â€œåˆ‡å“ªé‡Œâ€(åŒæ—¶ä¸ºåç»­æ¶ˆèå®éªŒåšå‡†å¤‡) 

```python
# ä¸‰ä¸ªæç¤ºè¯
text_prompts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]
inputs = processor(text=text_prompts, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    # å…³é”®æ­¥éª¤ï¼šè·å–æ–‡å­—ç‰¹å¾
    text_features = model.get_text_features(**inputs)
    text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
    
    # text_features[0] å¯¹åº” "a photo of a cat"
    cat_vector = text_features[0]

```

### 2.æ„å»ºä¸€ä¸ªç›¸ä¼¼åº¦çŸ©é˜µï¼Œçœ‹çœ‹çŒ«ã€ç‹—ã€é¸Ÿåœ¨clipæ¨¡å‹çœ¼ä¸­çš„ç›¸ä¼¼åº¦

å¦‚æœå¤ªè¿‡ç›¸ä¼¼åˆ™éœ€è¦éå¸¸ç»†å¾®çš„è°ƒæ•´(ç›´æ¥æ¢ä¸€ç§æ–¹æ³•) ï¼Œå¦‚æœæœ‰è¶³å¤Ÿå¤§çš„åŒºåˆ†åº¦åˆ™å¯ä»¥è€ƒè™‘å®šå‘åˆ‡é™¤(è¿›è¡Œä¸‹ä¸€æ­¥)è¿™ä¸€æ­¥ç”¨äº†ä¸€äº›aiè°ƒæ•´ä¸€ä¸‹è¾“å‡ºï¼Œæˆ‘çš„æ ¼å¼å®åœ¨æ˜¯æœ‰ç‚¹ä¸‘ï¼Œè¦å»å­¦å­¦ç¼©è¿›æ€ä¹ˆæ 

```python
#æ ¸å¿ƒåœ¨äºç®—ç›¸ä¼¼åº¦
similarity_matrix = text_features @ text_features.T
print("      [ çŒ« ]    [ ç‹— ]    [ é¸Ÿ ]")
names = ["çŒ«", "ç‹—", "é¸Ÿ"]

for i in range(3):
    row_str = "  ".join([f"{val:.4f}" for val in similarity_matrix[i]])
    print(f"{names[i]} : {row_str}")

```

### 3.ç¼–å†™æ‰‹æœ¯å‡½æ•°

ç»è¿‡ä»£ç æ£€éªŒåŒºåˆ†åº¦è¶³å¤Ÿå¤§ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰äº†è¦æ‰‹æœ¯çš„éƒ¨ä½äº†ç°åœ¨æˆ‘ä»¬æ¥å†™å…³é”®å‡½æ•°ï¼Œè¿™æ˜¯æ‰‹æœ¯çš„é€»è¾‘ã€‚ æ˜ç¡®æˆ‘ä»¬æ”¹å˜ç‰¹å¾å‘é‡çš„æ“ä½œç»å¯¹ä¸æ˜¯åŠ å™ª(å°¤å…¶æ˜¯ç‰¹å¾åˆ‡é™¤è¿™ä¸€å…³é”®é€»è¾‘)ï¼Œé¦–å…ˆæˆ‘ä»¬æ²¡æœ‰æŠŠ3000å¼ å›¾ç‰‡æ±¡æŸ“ï¼Œå¦‚æœå†æ¬¡è°ƒç”¨è¿™3000å¼ å›¾ç‰‡åˆ°stage1ä½ ä¹Ÿèƒ½å¾—åˆ°åŒæ ·ç»“æœã€‚å…¶æ¬¡æˆ‘ä»¬çš„é€»è¾‘å¹¶ä¸æ˜¯ç»™å›¾ç‰‡å’Œæç¤ºè¯å‡å»æˆ–æ˜¯åŠ ä¸Šä»»ä½•ä¸œè¥¿ï¼Œè€Œæ˜¯æ”¹å˜äº†512ä¸ªæƒé‡å€¼ 

```python
import torch
from PIL import Image

# 1. ç¡®ä¿æ‰‹æœ¯ç¯å¢ƒå°±ç»ª
text_prompts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]
inputs = processor(text=text_prompts, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    target_text_features = model.get_text_features(**inputs)

# 2. å®šä¹‰æˆ‘ä»¬çš„â€œæ‰‹æœ¯å‡½æ•°â€
# alpha å°±æ˜¯æ‰‹æœ¯å¼ºåº¦ï¼š0=ä¸å¼€åˆ€ï¼Œæ•°å­—è¶Šå¤§åˆ‡å¾—è¶Šç‹ 
def surgical_inference(image_path, alpha=0.0):
    #æ­£å¸¸çœ‹å›¾
    image = Image.open(image_path)
    inputs = processor(images=image, return_tensors="pt").to(device)

    with torch.no_grad():
        #è·å–åŸå§‹å›¾ç‰‡ç‰¹å¾
        image_features = model.get_image_features(**inputs)
        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

        #å…³é”®æ‰‹æœ¯æ­¥éª¤ï¼šç‰¹å¾åˆ‡é™¤
        if alpha > 0:
            #æ ¸å¿ƒé€»è¾‘ï¼šä»å›¾ç‰‡é‡Œå‡å» alpha å€çš„çŒ«ç‰¹å¾
            # cat_vector æ˜¯åœ¨ç¬¬ä¸€æ­¥æå–å‡ºæ¥çš„
            image_features = image_features - (alpha * cat_vector)

        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

        #è¯„åˆ†ç¯èŠ‚
        logits = image_features @ target_text_features.T
        probs = logits.softmax(dim=1) # è½¬åŒ–ä¸ºç™¾åˆ†æ¯”

    return probs[0] # è¿”å›ä¸‰ä¸ªç±»åˆ«çš„æ¦‚ç‡

```

### 4.ç”¨ç°å®æ ·æœ¬å¾—åˆ°Alpha(æ‰‹æœ¯å¼ºåº¦)

æˆ‘ä»¬å‡†å¤‡å¼€å§‹å¼€å§‹æ‰‹æœ¯ï¼Œä½†æ˜¯æˆ‘ä»¬ç°åœ¨åªçŸ¥é“åˆ‡å“ªé‡Œå’Œæ–¹æ³•ï¼Œä½†æ˜¯å°±åƒçœŸæ­£çš„ä¸´åºŠä¸€æ ·ï¼Œä¸åŒçš„æ‚£è€…è¦åˆ‡çš„åŠ›åº¦ä¸ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠalphaç†è§£ä¸ºæ‰‹æœ¯åˆ€çš„åŠ›åº¦ï¼Œå…ˆç”¨ç¥–ä¼ å›¾ç‰‡æœ‹å‹å®¶çš„å°çŒ«(test.jpg)æ¥è¯•è¯• 

#### å¤±è´¥ä¸€ï¼šä»£ç ç¼–å†™å‡ºé”™ï¼Œæ²¡æœ‰å½’ä¸€åŒ–ï¼Œè¿™ä¸€ç‚¹åœ¨ä¸Šé¢å°±æœ‰æ‰€ä½“ç°ï¼Œå¯¼è‡´è¾“å‡ºalpha=0æ—¶å°çŒ«çš„æ¦‚ç‡ä»…æœ‰30% 

```python
#æå‡ºç¥–ä¼ å›¾ç‰‡
test_image = "test.jpg"

# Alpha = 0
probs_before = surgical_inference(test_image, alpha=0.0)
print(f"Alpha=0 :")
print(f" Cat : {probs_before[0].item()*100:.2f}%")
print(f" Dog : {probs_before[1].item()*100:.2f}%")
print(f" Bird: {probs_before[2].item()*100:.2f}%")

# Alpha = 0.5
probs_after = surgical_inference(test_image, alpha=0.5)
print(f"Alpha=0.5 (å°è¯•åˆ‡é™¤):")
print(f"Cat : {probs_after[0].item()*100:.2f}%")
print(f"Dog : {probs_after[1].item()*100:.2f}%")
print(f"Bird: {probs_after[2].item()*100:.2f}%")

```

#### æ”¹æ­£ä¸€ï¼šç”¨å½’ä¸€åŒ–é‡æ–°ç¼–å†™ä»£ç ï¼Œå¾—åˆ°Alpha=0æ—¶ä¸€ä¸ªè¶…è¿‡90%çš„æ•°æ® 

```python
#é‡å†™æ‰‹æœ¯å‡½æ•°
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

local_path = r"D:\my_clip"
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained(local_path, weights_only=False).to(device)
processor = CLIPProcessor.from_pretrained(local_path)

text_prompts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]
inputs = processor(text=text_prompts, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    target_text_features = model.get_text_features(**inputs)
    target_text_features = target_text_features / target_text_features.norm(p=2, dim=-1, keepdim=True)

cat_vector = target_text_features[0]

#å®šä¹‰æ‰‹æœ¯å‡½æ•°
def surgical_inference(image_path, alpha):
    try:
        #è¯»å–å›¾ç‰‡
        image = Image.open(image_path)
        inputs = processor(images=image, return_tensors="pt").to(device)

        with torch.no_grad():
            #è·å–å›¾ç‰‡åŸå§‹ç‰¹å¾
            image_features = model.get_image_features(**inputs)
            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

            #å…³é”®æ‰‹æœ¯ï¼šç‰¹å¾å‡æ³•
            if alpha > 0:
                #å…¬å¼ï¼šæ–°ç‰¹å¾ = æ—§ç‰¹å¾ - (å¼ºåº¦ * çŒ«å‘é‡)
                image_features = image_features - (alpha * cat_vector)

            # æœ¯åç¼åˆï¼ˆå†æ¬¡å½’ä¸€åŒ–ï¼‰
            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

            #é‡æ–°æ‰“åˆ†
            logits = image_features @ target_text_features.T
            probs = logits.softmax(dim=1)

            return probs[0]
    except:
        return None

#è¿™æ˜¯é‡æ–°æµ‹è¯•ä»£ç 
test_image = "test.jpg"

# alpha=0
probs_0 = surgical_inference(test_image, alpha=0)
print(f"\nAlpha=0:")
print(f"çŒ«çš„å¯èƒ½æ€§: {probs_0[0].item()*100:.2f}%")

# alpha=0.3
probs_1 = surgical_inference(test_image, alpha=0.3)
print(f"\nAlpha=0.3:")
print(f"çŒ«çš„å¯èƒ½æ€§: {probs_1[0].item()*100:.2f}%")

# alpha=0.5
probs_2 = surgical_inference(test_image, alpha=0.5)
print(f"(Alpha=0.5:")
print(f"çŒ«çš„å¯èƒ½æ€§: {probs_2[0].item()*100:.2f}%")

```

### 5.ç”¨å•ä¸€æ ·æœ¬å¾—åˆ°ç²¾å‡†çš„Alpha

å¾—åˆ°çš„ç»“æœå‘ç°å½“Alpha=0.5æ—¶çŒ«çš„æ¦‚ç‡ä¸º74%ï¼Œæ²¡æœ‰è¾¾åˆ°è¦æ±‚çš„ç¨³å®šåœ¨60%ï¼Œäºæ˜¯æˆ‘åˆç”¨äºŒåˆ†æ³•çš„åŸç†ä¸€ééå°è¯•ï¼Œç»“æœå¾—åˆ°äº†Alpha=0.574ä¸ºè´´è¿‘è¦æ±‚çš„ï¼Œä½†æ˜¯è¿™ä¹Ÿä¸ºé”™è¯¯2åŸ‹ä¸‹äº†ä¼ç¬”ï¼Œæˆ‘è¿‡æ‹Ÿåˆäº† 

#### é”™è¯¯äºŒï¼šè¿‡æ‹Ÿåˆtest.jpg 

**Step1: ä¸€ä½å°æ•°å–æœ€æ¥è¿‘** 

```python
test_image = "test.jpg"
for alpha_try in [0.5, 0.6, 0.7, 0.8, 0.9]:
    prob = surgical_inference(test_image, alpha=alpha_try)
    cat_prob = prob[0].item() * 100
    print(f" Alpha = {alpha_try}: çŒ«çš„æ¦‚ç‡ = {cat_prob:.2f}%")

```

**Step2: ä¸¤ä½å°æ•°æœ€æ¥è¿‘** 

```python
test_image = "test.jpg"
# åœ¨ 0.5 åˆ° 0.6 ä¹‹é—´ï¼Œæ¯éš” 0.01 æˆ– 0.02 æµ‹ä¸€æ¬¡ï¼Œå› ä¸ºæ„Ÿè§‰å‰é¢çš„å¯èƒ½æ²¡é‚£ä¹ˆæ¥è¿‘
fine_grained_alphas = [0.52, 0.54, 0.55, 0.56, 0.57, 0.58]
for alpha_try in fine_grained_alphas:
    prob = surgical_inference(test_image, alpha=alpha_try)
    cat_prob = prob[0].item() * 100
    print(f" Alpha = {alpha_try}: çŒ«çš„æ¦‚ç‡ = {cat_prob:.2f}%")

```

**Step3ï¼šä¸‰ä½å°æ•°æœ€æ¥è¿‘** 

```python
import numpy as np

test_image = "test.jpg"
# ç”Ÿæˆä» 0.570 åˆ° 0.580 çš„åºåˆ—
nano_alphas = np.arange(0.570, 0.581, 0.001)

for alpha_try in nano_alphas:
    # å¼ºåˆ¶ä¿ç•™3ä½å°æ•°
    alpha_try = round(alpha_try, 3)

    prob = surgical_inference(test_image, alpha=alpha_try)
    cat_prob = prob[0].item() * 100

    print(f" Alpha = {alpha_try:.3f}: çŒ«çš„æ¦‚ç‡ = {cat_prob:.2f}%")

```

### 6.ç”¨CIFALæ ·æœ¬æ±‚ç®—Alpha

æ­£å½“æˆ‘ä»¥ä¸ºç»“æŸäº†ç›´æ¥æŠŠ0.574å¸¦å…¥å°±å¯ä»¥æ—¶ï¼Œç»“æœå´è®©æˆ‘å¤§è·Œçœ¼é•œï¼Œè¿™ç›´æ¥æŠŠçŒ«å…¨å¿˜äº†ï¼Œåªæœ‰2%çš„å‡†ç¡®ç‡äº†ï¼Œæˆ‘æ€»ç»“äº†ä¸€ä¸‹å¤±è´¥åŸå› ï¼Œåº”è¯¥æ˜¯æˆ‘çš„æµ‹è¯•å›¾è´¨é‡è¿‡äºé«˜äº†ï¼ŒçœŸæ­£CIFARæ•°æ®é›†é‡Œé¢çš„å›¾ç‰‡éƒ½æ˜¯éå¸¸ç³Šçš„ï¼Œå®Œå…¨å’Œæˆ‘çš„é«˜è´¨é‡å›¾ç‰‡æ²¡æ³•æ¯”ï¼Œæ‰€ä»¥æˆ‘æƒ³äº†ä¸€ä¸‹åº”è¯¥ç›´æ¥å¯¹åŸæ•°æ®é›†è¿›è¡Œæ¢¯åº¦ä¸‹é™ä»è€Œè·å¾—æœ€ä½³Alpha 

#### æ”¹æ­£2ï¼šç›´æ¥æ±‚å¯¹äºCIFARæ¥è¯´çš„æœ€ä½³Alpha 

**Step1: å…ˆéšæœºæ‹¿100å¼ è¯•è¯•** 

```python
import os
import torch
from PIL import Image
from tqdm import tqdm
from transformers import CLIPProcessor, CLIPModel

#å¦‚æœä½ ä¸‹è½½çš„åœ°å€æ•°æ®é›†åœ°å€ä¸æ˜¯è¿™ä¸ªçš„è¯è®°å¾—æ¢
dataset_path = r"D:\CIFAR_HF\test"

#å‡†å¤‡ç¯å¢ƒ
device = "cuda" if torch.cuda.is_available() else "cpu"
local_path = r"D:\my_clip"
model = CLIPModel.from_pretrained(local_path, weights_only=False).to(device)
processor = CLIPProcessor.from_pretrained(local_path)

#æå–çŒ«å‘é‡
text_prompts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]
inputs = processor(text=text_prompts, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    target_text_features = model.get_text_features(**inputs)
    target_text_features = target_text_features / target_text_features.norm(p=2, dim=-1, keepdim=True)
    cat_vector = target_text_features[0]

#å®šä¹‰æ‰‹æœ¯å‡½æ•°
def surgical_inference(image_path, alpha):
    try:
        image = Image.open(image_path)
        inputs = processor(images=image, return_tensors="pt").to(device)
        
        with torch.no_grad():
            image_features = model.get_image_features(**inputs)
            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

            if alpha > 0:
                image_features = image_features - (alpha * cat_vector)
                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

            #åŠ ä¸Šæ”¾å¤§é•œ (logit_scale) è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œä¸»è¦æ˜¯æŠŠçŒ«ç‹—é¸Ÿä¸‰è€…é—´çš„ç»†å¾®å·®åˆ«æ”¾å¤§ï¼Œå¹¶ä¸”æœ€åç”¨softmaxå½’ä¸€åŒ–å¹¶è¾“å‡ºç™¾åˆ†æ¯”
            logit_scale = model.logit_scale.exp()
            logits = logit_scale * image_features @ target_text_features.T
            probs = logits.softmax(dim=1)
            
            return probs[0]
    except:
        return None

#å¯»æ‰¾æœ€ä½³ Alpha
cat_folder = os.path.join(dataset_path, "cat")
cat_images = [os.path.join(cat_folder, f) for f in os.listdir(cat_folder)[:100]] # å–100å¼ æ ¡å‡†

best_alpha = 0
min_diff = 100

print(f"{'Alpha':<10} | {'å‡†ç¡®ç‡':<10} | {'è¯„ä»·'}")
print("-" * 40)

for alpha in [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]:
    correct = 0
    for img in cat_images:
        p = surgical_inference(img, alpha)
        if p is not None and p.argmax().item() == 0: # 0æ˜¯çŒ«
            correct += 1
            
    acc = (correct / len(cat_images)) * 100

    print(f"{alpha:<10} | {acc:.1f}%      | {flag}")

#ä½¿ç”¨æœ€ä½³å‚æ•°è·‘å…¨é‡æµ‹è¯•
classes = ['cat', 'dog', 'bird']
correct_counts = {"cat": 0, "dog": 0, "bird": 0}
total_counts = {"cat": 0, "dog": 0, "bird": 0}
all_files = []

# æ”¶é›†æ‰€æœ‰æ–‡ä»¶è·¯å¾„
for label_idx, name in enumerate(classes):
    folder = os.path.join(dataset_path, name)
    fnames = os.listdir(folder)[:1000]
    for f in fnames:
        all_files.append((os.path.join(folder, f), label_idx))

# è·‘è¿›åº¦æ¡
for img_path, label_idx in tqdm(all_files):
    label_name = classes[label_idx]
    total_counts[label_name] += 1

    probs = surgical_inference(img_path, best_alpha)
    if probs is not None and probs.argmax().item() == label_idx:
        correct_counts[label_name] += 1

# æœ€ç»ˆç»“æœ
final_cat = (correct_counts['cat'] / total_counts['cat']) * 100
final_dog = (correct_counts['dog'] / total_counts['dog']) * 100
final_bird = (correct_counts['bird'] / total_counts['bird']) * 100

print(f"Cat : {final_cat:.2f}%")
print(f"Dog : {final_dog:.2f}% ")
print(f"Bird: {final_bird:.2f}% ")

```

**Step2: ç”¨1000å¼ åšé«˜ç²¾åº¦æ ¡å‡†ï¼Œå¾—åˆ°Alphaåº”å–0.27** 

```python
import os
from tqdm import tqdm

# ä½ çš„å›¾ç‰‡è·¯å¾„
dataset_path = r"D:\CIFAR_HF\test"
cat_folder = os.path.join(dataset_path, "cat")
cat_images = []

if os.path.exists(cat_folder):
    fnames = os.listdir(cat_folder)[:1000] # å…¨é‡ 1000 å¼ 
    for fname in fnames:
        cat_images.append(os.path.join(cat_folder, fname))

# 0.25 æ˜¯ 62%ï¼Œæˆ‘ä»¬å¾€åè¯•
fine_alphas = [0.255, 0.260, 0.265, 0.270, 0.275, 0.280]
best_alpha = 0
min_diff = 100

for alpha in fine_alphas:
    correct = 0
    # è·‘ 1000 å¼ å›¾
    for img in cat_images:
        # å‡è®¾ surgical_inference è¿˜åœ¨å†…å­˜é‡Œç›´æ¥ç”¨
        # å¦‚æœæŠ¥é”™è¯´æ‰¾ä¸åˆ°ï¼Œè¯·é‡æ–°è¿è¡Œä¸Šä¸€æ®µä»£ç å®šä¹‰çš„å‡½æ•°
        probs = surgical_inference(img, alpha)
        if probs is not None and probs.argmax().item() == 0: # 0=cat
            correct += 1

    acc = (correct / 1000) * 100
    diff = abs(acc - 60)

    # æ ‡è®°æœ€æ¥è¿‘çš„ä¸€ä¸ª
    flag = ""
    if diff < min_diff:
        min_diff = diff
        best_alpha = alpha

    print(f"{alpha:<10} | {acc:.2f}%          | {diff:.2f} {flag}")
    
print(f" Alpha = {best_alpha}")

```

### 7.å¼€å±•æœ€åæµ‹è¯•

ç»“æœéå¸¸æˆåŠŸï¼Œå¯è§†åŒ–æˆ‘å’Œç¬¬å…«æ­¥çš„æ¶ˆèå®éªŒä¸€èµ·æ”¾åœ¨readmeæœ€ä¸‹æ–¹äº†
```python
import os
import torch
from PIL import Image
from tqdm import tqdm
from transformers import CLIPProcessor, CLIPModel
import matplotlib.pyplot as plt
import numpy as np

#ç”¨0.271
best_alpha = 0.271
dataset_path = r"D:\CIFAR_HF\test"#è®°å¾—æ”¹
device = "cuda" if torch.cuda.is_available() else "cpu"
local_path = r"D:\my_clip"

model = CLIPModel.from_pretrained(local_path, weights_only=False).to(device)
processor = CLIPProcessor.from_pretrained(local_path)

text_prompts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]
inputs = processor(text=text_prompts, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    target_text_features = model.get_text_features(**inputs)
    target_text_features = target_text_features / target_text_features.norm(p=2, dim=-1, keepdim=True)
    
cat_vector = target_text_features[0] # é”å®šçŒ«

def surgical_inference(image_path, alpha):
    try:
        image = Image.open(image_path)
        inputs = processor(images=image, return_tensors="pt").to(device)
        
        with torch.no_grad():
            image_features = model.get_image_features(**inputs)
            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

            if alpha > 0:
                image_features = image_features - (alpha * cat_vector)
                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)

            # æ”¾å¤§é•œä¹‹å‰æè¿‡çš„
            logit_scale = model.logit_scale.exp()
            logits = logit_scale * image_features @ target_text_features.T
            probs = logits.softmax(dim=1)
            
            return probs[0]
    except:
        return None

classes = ['cat', 'dog', 'bird']
correct_counts = {"cat": 0, "dog": 0, "bird": 0}
total_counts = {"cat": 0, "dog": 0, "bird": 0}

# æ”¶é›†æ–‡ä»¶è·¯å¾„
all_files = []
for label_idx, name in enumerate(classes):
    folder = os.path.join(dataset_path, name)
    if os.path.exists(folder):
        fnames = os.listdir(folder)[:1000] # æ¯ä¸ªç±»å–1000å¼ 
        for f in fnames:
            all_files.append((os.path.join(folder, f), label_idx))

# è¿›åº¦æ¡è·‘èµ·æ¥
for img_path, label_idx in tqdm(all_files):
    label_name = classes[label_idx]
    total_counts[label_name] += 1

    probs = surgical_inference(img_path, best_alpha)

    if probs is not None:
        prediction = probs.argmax().item()
        if prediction == label_idx:
            correct_counts[label_name] += 1

# è®¡ç®—æœ€ç»ˆå¾—åˆ†
final_scores = [
    (correct_counts['cat'] / total_counts['cat']) * 100,
    (correct_counts['dog'] / total_counts['dog']) * 100,
    (correct_counts['bird'] / total_counts['bird']) * 100
]

print(f"Cat : {final_scores[0]:.2f}%")
print(f"Dog : {final_scores[1]:.2f}%")
print(f"Bird: {final_scores[2]:.2f}%")

```

### 8.åšä¸¤ä¸ªæ¶ˆèå®éªŒ
#### ç¬¬ä¸€ä¸ªå®éªŒçš„ç›®çš„æ˜¯çœ‹çœ‹æˆ‘ä»¬æœ‰æ²¡æœ‰åˆ‡ä¸­512ä¸ªæƒé‡çš„æ ¸å¿ƒï¼Œä»¥æ­¤è¯æ˜æˆ‘ä¸æ˜¯ä¹±åˆ‡çš„ï¼š 

é¦–å…ˆæˆ‘è¦æ˜ç¡®æ ¸å¿ƒæƒé‡æ˜¯ä»€ä¹ˆï¼šç»å¯¹å€¼è¶Šå¤§çš„æƒé‡è¶Šæ ¸å¿ƒï¼Œè¶Šæ¥è¿‘0è¶Šä¸é‡è¦ï¼Œè¿™æ¶‰åŠåˆ°aiç®—åˆ†çš„åº•å±‚åŸç†ç‚¹ä¹˜ï¼Œå³æ€»åˆ†=(ç‰¹å¾1*æƒé‡1)+(ç‰¹å¾1*æƒé‡1)+......+(ç‰¹å¾512*æƒé‡512)ï¼Œå¦‚æœè¯´ä¸€ä¸ªç‰¹å¾æ ¹æœ¬æ¯«æ— ç”¨å¤„ï¼Œé‚£ä¹ˆå®ƒå¯¹æ€»åˆ†çš„å½±å“å°±åº”è¯¥å¾ˆå°ç”šè‡³ä¸ºé›¶å°±ä¸ä¼šå½±å“æ€»åˆ†ã€‚åä¹‹å¦‚æœä¸€ä¸ªç‰¹å¾éå¸¸é‡è¦ï¼Œé‚£ä¹ˆä»–å°±åº”è¯¥å¯¹æ€»åˆ†æœ‰å¾ˆå¤§å½±å“(åŠ å¾ˆå¤šåˆ†æˆ–å‡å¾ˆå¤šåˆ†)ï¼Œè‡ªç„¶æƒé‡çš„ç»å¯¹å€¼å°±åº”è¯¥å¾ˆå¤§ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬é€‰çš„æ ¸å¿ƒæƒé‡æ˜¯ç»å¯¹å€¼éå¸¸å¤§çš„ï¼Œè€Œéæ ¸å¿ƒæ˜¯æ¥è¿‘0çš„ 

æˆ‘è®¾ç½®äº†ä¸‰ä¸ªç»„åˆ«ï¼Œç¬¬ä¸€ç»„æ˜¯éšæœºé€‰512ä¸­çš„50%æƒé‡ï¼Œç¬¬äºŒç»„æ˜¯æ”¹éæ ¸å¿ƒçš„512ä¸­çš„50%æƒé‡ï¼Œç¬¬ä¸‰ç»„æ˜¯æ”¹512ä¸­çš„50%æ ¸å¿ƒæƒé‡ã€‚å¦‚æœç¬¬ä¸‰ç»„è¯†åˆ«æ¦‚ç‡<ç¬¬ä¸€ç»„<ç¬¬äºŒç»„ï¼Œé‚£ä¹ˆå°±å¯ä»¥è¯æ˜æˆ‘å¹¶éä¹±åˆ‡çš„ 

#### ç¬¬äºŒä¸ªå®éªŒçš„ç›®çš„æ˜¯ä¸ºäº†è¯æ˜æˆ‘æ²¡æœ‰æš´åŠ›æ”¹æƒé‡ 

å¦‚æœæˆ‘æ˜¯ä¹±æ”¹çš„ï¼Œé‚£ä¹ˆå½“Alphaæ…¢æ…¢ä¸‹é™æ—¶ï¼Œå‡†ç¡®ç‡ä¸ä¼šçº¿æ€§ä¸‹é™ï¼Œè€Œæ˜¯å¾ˆæœ‰å¯èƒ½ä¸Šä¸‹ä¹±åŠ¨ï¼Œæˆ‘ä»¬ç›´æ¥è®¡ç®—å‡ ä¸ªAlphaä»¥åŠå…¶å¯¹åº”çš„å‡†ç¡®ç‡å€¼ï¼Œç»˜å›¾çœ‹çœ‹R^2å€¼å°±è¡Œäº†ï¼Œå¦‚æœR^2å€¼æ¯”è¾ƒå¤§é‚£ä¹ˆå°±è¯æ˜æˆ‘æ²¡æœ‰ä¹±æ”¹æƒé‡ 

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

# --- 1. å‡†å¤‡ç¯å¢ƒ ---
dataset_path = r"D:\CIFAR_HF\test"  # ä½ çš„è·¯å¾„
best_alpha = 0.271
device = "cuda" if torch.cuda.is_available() else "cpu"

if 'model' not in globals():
    local_path = r"D:\my_clip"
    model = CLIPModel.from_pretrained(local_path, weights_only=False).to(device)
    processor = CLIPProcessor.from_pretrained(local_path)

# æå–çŒ«å‘é‡
text_prompts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]
inputs = processor(text=text_prompts, return_tensors="pt", padding=True).to(device)

with torch.no_grad():
    target_text_features = model.get_text_features(**inputs)
    target_text_features = target_text_features / target_text_features.norm(p=2, dim=-1, keepdim=True)
    cat_vector = target_text_features[0]

# å‡†å¤‡ 500 å¼ çŒ«å›¾
cat_folder = os.path.join(dataset_path, "cat")
cat_images = [os.path.join(cat_folder, f) for f in os.listdir(cat_folder)[:500]]

# æ‰‹æœ¯å‡½æ•° (å¤ç”¨)
def surgical_inference_masked(image_path, alpha, mask):
    try:
        image = Image.open(image_path)
        inputs = processor(images=image, return_tensors="pt").to(device)
        
        with torch.no_grad():
            image_features = model.get_image_features(**inputs)
            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
            
            if alpha > 0:
                mask_tensor = torch.tensor(mask, device=device).float().unsqueeze(0)
                intervention = alpha * cat_vector * mask_tensor
                image_features = image_features - intervention
                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
                
            logit_scale = model.logit_scale.exp()
            logits = logit_scale * image_features @ target_text_features.T
            return logits.softmax(dim=1)[0]
    except: 
        return None

ratios = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0] # 0% åˆ° 100%
acc_curve = []

for r in ratios:
    # ç”Ÿæˆéšæœºæ©ç  (ä¿ç•™ r% çš„é€šé“ä¸º 1)
    mask = np.zeros(512)
    if r > 0:
        indices = np.random.choice(512, int(512 * r), replace=False)
        mask[indices] = 1

    # è·‘æµ‹è¯•
    correct = 0
    for img in cat_images: # ä¸ç”¨ tqdm åˆ·å±äº†ï¼Œé™é»˜è·‘
        p = surgical_inference_masked(img, best_alpha, mask)
        if p is not None and p.argmax().item() == 0:
            correct += 1
            
    acc = (correct / len(cat_images)) * 100
    acc_curve.append(acc)
    print(f"  - å¹²é¢„æ¯”ä¾‹ {int(r*100)}%: å‡†ç¡®ç‡ {acc:.1f}%")

ratio_50 = 256
# 1. æ‰¾å‡ºçŒ«å‘é‡é‡Œç»å¯¹å€¼æœ€å¤§çš„ 256 ä¸ªé€šé“ (Top-k)
# è¿™äº›é€šé“ä»£è¡¨äº†â€œçŒ«â€æœ€æ˜¾è‘—çš„ç‰¹å¾
values, indices = torch.topk(cat_vector.abs(), 512) # å…ˆå…¨æ’ä¸ªåº
top_indices = indices[:256].cpu().numpy()
bottom_indices = indices[-256:].cpu().numpy()

# 2. åˆ¶ä½œä¸‰ç§æ©ç 
mask_top = np.zeros(512); mask_top[top_indices] = 1
mask_bottom = np.zeros(512); mask_bottom[bottom_indices] = 1
mask_random = np.zeros(512); mask_random[np.random.choice(512, 256, replace=False)] = 1

# 3. è·‘æµ‹è¯•
def run_test(mask, name):
    correct = 0
    for img in tqdm(cat_images, desc=name):
        p = surgical_inference_masked(img, best_alpha, mask)
        if p is not None and p.argmax().item() == 0:
            correct += 1
    return (correct / len(cat_images)) * 100

acc_top = run_test(mask_top, "Top-50% (é‡è¦ç‰¹å¾)")
acc_bottom = run_test(mask_bottom, "Bottom-50% (éé‡è¦ç‰¹å¾)")
acc_random = acc_curve[2] # ç›´æ¥å–åˆšæ‰è·‘è¿‡çš„ 40% æˆ– 60% é™„è¿‘çš„è¿‘ä¼¼å€¼ï¼Œæˆ–è€…é‡è·‘
acc_random = run_test(mask_random, "Random-50% (éšæœº)")

```

### 9.å¤ç°ä»£ç æ±‡æ€»(å¯ç›´æ¥ç”¨è¯¥ä»£ç å¤ç°)

åœ†æ»¡æˆåŠŸå“ˆï¼Œæœ€åæ€»ç»“ä¸€ä¸‹ä»£ç ï¼Œå†ç”»ä¸‰å¼ å›¾çœ‹çœ‹ï¼Œå¤ç°æ—¶ç›´æ¥è¿è¡Œè¿™ä¸²ä»£ç å°±è¡Œ(å‰ææ˜¯æœ‰è·Ÿç€stage1é…ç½®ç¯å¢ƒä»¥åŠä¸‹è½½æ¨¡å‹ä¸æ•°æ®) 

```python
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm
from transformers import CLIPProcessor, CLIPModel

CONFIG = {
    "alpha": 0.271,                    # æœ€ç»ˆæ‰‹æœ¯å¼ºåº¦
    "dataset_path": r"D:\CIFAR_HF\test", # æ•°æ®è·¯å¾„
    "model_path": r"D:\my_clip",       # æ¨¡å‹è·¯å¾„
    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ (ä¼˜å…ˆä½¿ç”¨ GPU)
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "sample_size": 1000,               # ä¸»æµ‹è¯•æ ·æœ¬æ•° (æ¯ç±»)
    "ablation_size": 500               # æ¶ˆèå®éªŒæ ·æœ¬æ•° (èŠ‚çœæ—¶é—´)
}

# ç”»å›¾çš„é…è‰²
COLORS = {
    "red": "#E64B35", "blue": "#1F4E79", "glacier": "#E9F2F9",
    "base": "#DCDDE1", "dark": "#2F3640", "orange": "#F39C12"
}

# 2. æ ¸å¿ƒå¼•æ“å‡†å¤‡ (Engine Setup)
# åŠ è½½æ¨¡å‹ (å¦‚æœå†…å­˜ä¸­å·²æœ‰åˆ™å¤ç”¨ï¼Œé˜²æ­¢é‡å¤åŠ è½½)
if 'model' not in globals() or model.device.type != CONFIG['device']:
    model = CLIPModel.from_pretrained(CONFIG['model_path'], weights_only=False).to(CONFIG['device'])
    processor = CLIPProcessor.from_pretrained(CONFIG['model_path'])
else:
    print("æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡åŠ è½½æ­¥éª¤ã€‚")

# æå–æ‰‹æœ¯åˆ€ (Cat Vector)
text_prompts = ["a photo of a cat", "a photo of a dog", "a photo of a bird"]
inputs = processor(text=text_prompts, return_tensors="pt", padding=True).to(CONFIG['device'])

with torch.no_grad():
    text_feats = model.get_text_features(**inputs)
    text_feats /= text_feats.norm(p=2, dim=-1, keepdim=True)
    cat_vector = text_feats[0] # é”å®šçŒ«å‘é‡

# æ ¸å¿ƒä¿®å¤ç‚¹ï¼šæ‰‹æœ¯æ¨ç†å‡½æ•° 
def surgical_inference(img_path, alpha, mask=None):
    """æ‰‹æœ¯æ¨ç†æ ¸å¿ƒå‡½æ•° (ä¿®å¤äº†æ•°æ®ç±»å‹ä¸åŒ¹é…çš„Bug)"""
    try:
        image = Image.open(img_path)
        inputs = processor(images=image, return_tensors="pt").to(CONFIG['device'])
        
        with torch.no_grad():
            img_feats = model.get_image_features(**inputs)
            img_feats /= img_feats.norm(p=2, dim=-1, keepdim=True)

            # --- å¹²é¢„é€»è¾‘ ---
            if alpha > 0:
                intervention = alpha * cat_vector
                if mask is not None:
                    m_tensor = torch.tensor(mask, device=CONFIG['device'], dtype=img_feats.dtype).unsqueeze(0)
                    intervention = intervention * m_tensor

                # æ‰§è¡Œå‡æ³•æ‰‹æœ¯
                img_feats = (img_feats - intervention)
                # æœ¯åé‡æ–°å½’ä¸€åŒ–
                img_feats /= img_feats.norm(p=2, dim=-1, keepdim=True)
            # ----------------

            logits = model.logit_scale.exp() * img_feats @ text_feats.T
            return logits.softmax(dim=1)[0]
    except Exception as e:
        # print(f"Error processing {img_path}: {e}") # è°ƒè¯•æ—¶å¯æ‰“å¼€
        return None

#å®éªŒ A: ä¸»ä»»åŠ¡ (å…¨é‡å¯¹æ¯”)
results_baseline = [87.8, 90.7, 93.2] # å¼•ç”¨ Phase 1 åŸºçº¿æ•°æ®
results_ours = []
classes = ['cat', 'dog', 'bird']

for idx, name in enumerate(classes):
    folder = os.path.join(CONFIG['dataset_path'], name)
    #ç¡®ä¿åªè¯»å–å›¾ç‰‡æ–‡ä»¶
    imgs = [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:CONFIG['sample_size']]

    correct = 0
    for img in tqdm(imgs, desc=f"Testing {name.capitalize()}"):
        p = surgical_inference(img, CONFIG['alpha'])
        if p is not None and p.argmax().item() == idx:
            correct += 1
            
    results_ours.append((correct / len(imgs)) * 100)

#å‡†å¤‡æ¶ˆèå®éªŒæ•°æ®
cat_folder = os.path.join(CONFIG['dataset_path'], 'cat')
cat_imgs = [os.path.join(cat_folder, f) for f in os.listdir(cat_folder) if f.lower().endswith(('.png', '.jpg'))][:CONFIG['ablation_size']]

#å®éªŒ B: æ¢¯åº¦æ‰«æ (Curve Fitting)
print(f"ğŸ“ˆ [å®éªŒ B] å¯åŠ¨æ¢¯åº¦å“åº”æ‰«æ (æ ·æœ¬æ•°: {len(cat_imgs)})...")
grad_x = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
grad_y = []

for r in grad_x:
    #ç”Ÿæˆéšæœºæ©ç  (ræ¯”ä¾‹ä¸º1ï¼Œå…¶ä½™ä¸º0)
    mask = np.zeros(512, dtype=np.float32)
    if r > 0:
        indices = np.random.choice(512, int(512 * r), replace=False)
        mask[indices] = 1.0

    correct = 0
    #ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ï¼Œç¡®ä¿æ²¡å¡æ­»
    for img in tqdm(cat_imgs, desc=f"Ratio {r:.1f}", leave=False):
        p = surgical_inference(img, CONFIG['alpha'], mask)
        if p is not None and p.argmax().item() == 0:
            correct += 1
            
    grad_y.append((correct / len(cat_imgs)) * 100)

#å®éªŒ C: é‡è¦æ€§æ¶ˆè (Top vs Bot)
vals, idxs = torch.topk(cat_vector.abs(), 512)
top_idx = idxs[:256].cpu().numpy()
bot_idx = idxs[256:].cpu().numpy()

mask_top = np.zeros(512, dtype=np.float32); mask_top[top_idx] = 1.0
mask_bot = np.zeros(512, dtype=np.float32); mask_bot[bot_idx] = 1.0

def run_ablation_test(mask, desc):
    c = 0
    for img in tqdm(cat_imgs, desc=desc, leave=False):
        p = surgical_inference(img, CONFIG['alpha'], mask)
        if p is not None and p.argmax().item() == 0: c += 1
    return (c / len(cat_imgs)) * 100

acc_top = run_ablation_test(mask_top, "Top-50%")
acc_bot = run_ablation_test(mask_bot, "Bottom-50%")
acc_rnd = grad_y[2] # ä½¿ç”¨æ¢¯åº¦å®éªŒä¸­ 40% æˆ– 60% çš„è¿‘ä¼¼å€¼ä½œä¸ºéšæœºåŸºçº¿ (æˆ–è€…å–ä¸­é—´å€¼)

# ä¸ºäº†ä¸¥è°¨ï¼Œè¿™é‡Œç”¨ 50% éšæœºé‡è·‘ä¸€æ¬¡
mask_rnd = np.zeros(512, dtype=np.float32); mask_rnd[np.random.choice(512, 256, replace=False)] = 1.0
acc_rnd = run_ablation_test(mask_rnd, "Random-50%")

print("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–å›¾ç‰‡")
plt.rcParams['figure.dpi'] = 150
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['axes.unicode_minus'] = False

fig = plt.figure(figsize=(21, 7))
gs = fig.add_gridspec(1, 3)

# é€šç”¨å»æ¡†å‡½æ•°
def despine_ax(ax):
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_color('#CCCCCC')
    ax.spines['bottom'].set_color('#CCCCCC')

# --- Panel A: Targeted Suppression ---
ax1 = fig.add_subplot(gs[0, 0])
x = np.arange(3)
width = 0.35
ax1.bar(x - width/2, results_baseline, width, label='Baseline', color=COLORS['base'], alpha=0.7)
ax1.bar(x + width/2, results_ours, width, label='Ours ($\\alpha=0.271$)', color=COLORS['red'])

for i in range(3):
    ax1.text(x[i]-width/2, results_baseline[i]+1, f'{results_baseline[i]}', ha='center', fontsize=9)
    ax1.text(x[i]+width/2, results_ours[i]+1, f'{results_ours[i]:.1f}', ha='center', fontsize=10, fontweight='bold', color=COLORS['red'])
    delta = results_ours[i] - results_baseline[i]
    d_color = COLORS['red'] if delta < 0 else COLORS['blue']
    ax1.text(x[i], max(results_baseline[i], results_ours[i])+15, f"{delta:+.1f}%", ha='center', weight='bold',
             color='white', fontsize=9, bbox=dict(facecolor=d_color, edgecolor='none', boxstyle='round,pad=0.3'))

ax1.set_title('A. Targeted Suppression Result', loc='left', fontsize=14, fontweight='bold', pad=25)
ax1.set_ylabel('Accuracy (%)', fontweight='bold'); ax1.set_xticks(x)
ax1.set_xticklabels(['Cat', 'Dog', 'Bird'], fontweight='bold')
ax1.set_ylim(0, 135); ax1.legend(frameon=False, loc='upper left'); despine_ax(ax1)

# --- Panel B: Gradient Response ---
ax2 = fig.add_subplot(gs[0, 1])

# è®¡ç®— R2
z = np.polyfit(grad_x, grad_y, 1)
p_poly = np.poly1d(z)
y_fit = p_poly(grad_x)
ss_res = np.sum((grad_y - y_fit) ** 2)
ss_tot = np.sum((grad_y - np.mean(grad_y)) ** 2)
r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0 # é˜²æ­¢é™¤é›¶

ax2.plot(grad_x, grad_y, color=COLORS['blue'], marker='o', markersize=8, linewidth=3, zorder=5)
ax2.fill_between(grad_x, grad_y, min(grad_y)-5, color=COLORS['glacier'], alpha=0.6, zorder=1)
ax2.text(0.05, min(grad_y)+5, f'$R^2 = {r2:.3f}$\nLinear Decay', fontsize=11, fontweight='bold',
         color=COLORS['blue'], bbox=dict(facecolor='white', edgecolor=COLORS['blue'], boxstyle='round,pad=0.5', alpha=0.8))

# æ•°æ®è¡¨
table_data = [[f"{r:.1f}", f"{v:.1f}%"] for r, v in zip(grad_x, grad_y)]
table = ax2.table(cellText=table_data, colLabels=['Ratio', 'Acc.'], loc='upper right', bbox=[0.7, 0.65, 0.28, 0.32])
table.auto_set_font_size(False); table.set_fontsize(8)
for (row, col), cell in table.get_celld().items(): cell.set_edgecolor('#DDDDDD')

ax2.set_title('B. Control Sensitivity Analysis', loc='left', fontsize=14, fontweight='bold', pad=25)
ax2.set_xlabel('Intervention Ratio', fontweight='bold'); ax2.set_ylim(min(grad_y)-10, 100); despine_ax(ax2)

# --- Panel C: Feature Sparsity ---
ax3 = fig.add_subplot(gs[0, 2])
bars = ax3.bar(['Top 50%', 'Random', 'Bottom 50%'], [acc_top, acc_rnd, acc_bot], color=[COLORS['red'], COLORS['orange'], COLORS['blue']], width=0.6)

for b in bars: ax3.text(b.get_x()+b.get_width()/2, b.get_height()+2, f'{b.get_height():.1f}%', ha='center', weight='bold')

ax3.plot([0, 0, 2, 2], [92, 95, 95, 92], lw=1.5, color=COLORS['dark'])
ax3.text(1, 96, f'$\\Delta = {acc_bot-acc_top:.1f}\\%$', ha='center', weight='bold')
ax3.set_title('C. Feature Sparsity Mechanism', loc='left', fontsize=14, fontweight='bold', pad=25)
ax3.set_ylim(0, 110); despine_ax(ax3)

plt.subplots_adjust(left=0.08, right=0.95, top=0.85, bottom=0.15, wspace=0.3)
plt.show()

```
![å®éªŒç»“æœå¯è§†åŒ–](Figure_1.png)
